# 关键术语

监督学习
 + 分类
 + 回归：预测数值型数据

 监督学习一般使用两种类型的目标变量：**标称型**和**数值型**

 将属性称为**特征**

 将分类问题中的目标变量称为**类别**

 特征或属性通常是训练样本集的列，它们是独立测量得到的结果，多个特征联系在一起共同组成一个训练样本

 为了测试机器学习算法的效果，通常使用两套独立的样本集：**训练数据**和**测试数据**。当机器学习开始运行时，使用训练样本集作为算法的输入，训练完成之后输入测试样本。输入测试样本时并不提供测试样本的目标变量，由程序决定样本属于哪个类别。比较测试样本预测的目标变量值与实际样本类别之间的差别，就可以得出算法的实际精确度。


# 机器学习的主要任务

 用于执行分类、回归、聚类和密度估计的机器学习算法

 监督学习的用途

 &nbsp; | &nbsp; 
 -------|-------
 k-近邻算法|线性回归
 朴素贝叶斯算法|局部加权线性回归
 支持向量机|Ridge 回归
 决策树|Lasso 最小回归系数估计

 无监督学习的用途

 &nbsp; | &nbsp;
 -------|--------
 k-均值|最大期望算法
 DBSCAN|Parzen 窗设计


 # 如何选择合适的算法

 需要考虑如下两个问题：

  + 使用机器学习算法的目的，想要算法完成何种任务
  + 需要分析或收集的数据是什么


 使用机器学习算法的目的

  + 想要预测目标变量的值：**监督学习算法**

  	- 离散型目标变量 : **分类算法**
  	- 连续型目标变量 : **回归算法**

  + 不想预测目标变量的值：**无监督学习算法**

    - 是否需要将数据划分为离散的组
       
       * 是。**聚类算法**
       * 还需要估计数据与每个分组的相似程度。**密度估计算法**


还需要考虑数据问题。应了解数据的以下特性：

  + 特征值是离散型变量还是连续型变量
  + 特征值中是否存在缺失的值，何种原因造成缺失值
  + 数据中是否存在异常值，某个特征发生的频率如何
  + ... ...


# 开发机器学习应用程序的步骤

 + 收集数据。使用很多方法收集样本数据
 + 准备输入数据。
 + 分析输入数据。确保数据中没有垃圾数据
 + 训练算法。
 + 测试算法。测试算法工作的效果，如果不满意算法的输出结果，则可以回到第4步，改正并加以测试
 + 使用算法。

 # K-近邻算法

  + 优点：精度高、对异常值不敏感、无数据输入假定
  + 缺点：计算复杂度高、空间复杂度高
  + 使用数据范围：数值型和标称型

  K-近邻算法的一般流程
  
  + 收集数据：可以使用任何方法
  + 准备数据：距离计算所需要的数值，最好是结构化的数据格式
  + 分析数据：可以使用任何方法
  + 训练算法：此步骤不适用于 k-近邻算法
  + 测试算法：计算错误率
  + 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行 k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理


# k-近邻算法小结

k-近邻算法是分类数据最简单最有效的算法。k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时

k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实；例样本和典型实例样本具有什么特征。