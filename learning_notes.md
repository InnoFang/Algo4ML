# 关键术语

监督学习
 + 分类
 + 回归：预测数值型数据

 监督学习一般使用两种类型的目标变量：**标称型**和**数值型**

 将属性称为**特征**

 将分类问题中的目标变量称为**类别**

 特征或属性通常是训练样本集的列，它们是独立测量得到的结果，多个特征联系在一起共同组成一个训练样本

 为了测试机器学习算法的效果，通常使用两套独立的样本集：**训练数据**和**测试数据**。当机器学习开始运行时，使用训练样本集作为算法的输入，训练完成之后输入测试样本。输入测试样本时并不提供测试样本的目标变量，由程序决定样本属于哪个类别。比较测试样本预测的目标变量值与实际样本类别之间的差别，就可以得出算法的实际精确度。


# 机器学习的主要任务

 用于执行分类、回归、聚类和密度估计的机器学习算法

 监督学习的用途

 &nbsp; | &nbsp; 
 -------|-------
 k-近邻算法|线性回归
 朴素贝叶斯算法|局部加权线性回归
 支持向量机|Ridge 回归
 决策树|Lasso 最小回归系数估计

 无监督学习的用途

 &nbsp; | &nbsp;
 -------|--------
 k-均值|最大期望算法
 DBSCAN|Parzen 窗设计


# 如何选择合适的算法

 需要考虑如下两个问题：

  + 使用机器学习算法的目的，想要算法完成何种任务
  + 需要分析或收集的数据是什么


 使用机器学习算法的目的

  + 想要预测目标变量的值：**监督学习算法**

  	- 离散型目标变量 : **分类算法**
  	- 连续型目标变量 : **回归算法**

  + 不想预测目标变量的值：**无监督学习算法**

    - 是否需要将数据划分为离散的组
       
       * 是。**聚类算法**
       * 还需要估计数据与每个分组的相似程度。**密度估计算法**


还需要考虑数据问题。应了解数据的以下特性：

  + 特征值是离散型变量还是连续型变量
  + 特征值中是否存在缺失的值，何种原因造成缺失值
  + 数据中是否存在异常值，某个特征发生的频率如何
  + ... ...


# 开发机器学习应用程序的步骤

 + 收集数据。使用很多方法收集样本数据
 + 准备输入数据。
 + 分析输入数据。确保数据中没有垃圾数据
 + 训练算法。
 + 测试算法。测试算法工作的效果，如果不满意算法的输出结果，则可以回到第4步，改正并加以测试
 + 使用算法。

# K-近邻算法

  + 优点：精度高、对异常值不敏感、无数据输入假定
  + 缺点：计算复杂度高、空间复杂度高
  + 使用数据范围：数值型和标称型

  K-近邻算法的一般流程
  
  + 收集数据：可以使用任何方法
  + 准备数据：距离计算所需要的数值，最好是结构化的数据格式
  + 分析数据：可以使用任何方法
  + 训练算法：此步骤不适用于 k-近邻算法
  + 测试算法：计算错误率
  + 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行 k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理


## k-近邻算法小结

k-近邻算法是分类数据最简单最有效的算法。k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时

k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实；例样本和典型实例样本具有什么特征。


# 决策树

 k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解

 决策树的一个重要任务是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。

 ## 决策树的构造

 + 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
 + 缺点：可能产生过度匹配问题
 + 使用数据类型：标称型和数值型

 在构造决策树时，需要找到哪个特征在划分数据分类时起决定性作用，为了找到决定性特征，划分出最好的结果，我们需要评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程，直到所有具有相同类型的数据均在一个数据子集内


 决策树的一般流程
  + 收集数据：可以使用任何方法
  + 输入数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
  + 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
  + 训练算法：构造树的数据结构
  + 测试算法：使用经验树计算错误率
  + 使用算法：此步骤可以适用于任何监督学习算法那，而使用决策树可以更好地理解数据的内在含义


## 使用 ID3 算法划分数据集

### 算法介绍

> 该算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类

> 在信息论中，期望信息越小，那么信息增益就越大，从而纯度就越高。

> ID3算法的核心思想就是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分裂。

> 该算法采用自顶向下的贪婪搜索遍历可能的决策空间。
 
### 信息熵和信息增益

#### 信息熵(Entropy)

  **熵**在物理学中的概念表示一个系统的无序程度，而在信息论中则表示对不确定性的度量。道理是相似的，信息熵越低，则系统越有序，反之，信息熵越高，则系统越混乱。


  假如一个随机变量 X 的取值为 X={x1,x2,...,xn}，每一种取到的概率分别是 {p1,p2,...,pn}，那么 X 的熵定义为

  ![](http://images.cnitblog.com/blog/571227/201412/112112589313898.png)


  对于**分类系统**而言，将类别表示为 C，那么 C1，C2,...,Cn，每一个类别出现的概率就是 `P(C1),P(C2),...,P(Cn)`。n 表示类别总数。此时，分类系统的熵，即**信息熵**就可以表示为

  ![](http://images.cnitblog.com/blog/571227/201412/112123167755487.png)


#### 信息增益(Information gain)

  对每个特征而言，系统有它和没它时的信息量各是多少，两者的差值就是这个特征给系统带来的信息量。简单的说，信息增益指的是在进行属性划分前后信息的差值

  举个例子，以**是否买电脑**为例，下面给出一个表，学习目标是**买**或者**不买**

id |  年龄 | 收入 |  学生 | 信用率 | 类别：是否买电脑
---|-------|-----|-------|--------|---------------
 1 | 年轻  | 高   |  否   | 一般   | 否
 2 | 年轻  | 高   |  否   | 极好   | 否
 3 | 中年  | 高   |  否   | 一般   | 是   
 4 | 年长  | 中   |  否   | 一般   | 是   
 5 | 年长  | 低   |  是   | 一般   | 是   
 6 | 年长  | 低   |  是   | 极好   | 否
 7 | 中年  | 低   |  是   | 极好   | 是
 8 | 年轻  | 中   |  否   | 一般   | 否
 9 | 年轻  | 低   |  是   | 一般   | 是   
10 | 年长  | 中   |  是   | 一般   | 是   
11 | 年轻  | 中   |  是   | 极好   | 是
12 | 中年  | 中   |  否   | 极好   | 是
13 | 中年  | 高   |  是   | 一般   | 是   
14 | 年长  | 中   |  否   | 极好   | 否

一共 14 个样例，包括 5 个负例和 9 个正例。那么信息熵计算如下

![](http://images.cnitblog.com/blog/571227/201412/121404563217970.png)

接下来，我们假设按年龄（age）进行分类，其年龄有三个类别：年轻（youth）、中年（middle age）、年长（senior），那么对应的年龄与是否购买电脑的关系大致如下：

``` 
			age	
  ___________|___________     
  |          |          |
youth	middle age 	  senior
  |          |          |
-----      -----      -----  
  no        yes 	   yes
  no        yes 	   yes
  no        yes 	   no
  yes       yes 	   yes
  yes       yes 	   no
```

划分后，各个分支的信息熵计算如下

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbqvyrj30n301qaa3.jpg)

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbirtvj30mc01ndfu.jpg)

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbn4h5j30nh01oaa3.jpg)

划分后的信息熵为

![](http://images.cnitblog.com/blog/571227/201412/121424414622037.png)

![](http://images.cnitblog.com/blog/571227/201412/121425239464583.png) 代表在特征属性 `T` 的条件下样本的**条件熵**。那么最终得到特征属性 `T` 带来的信息增益为

![](http://images.cnitblog.com/blog/571227/201412/121433352752920.png)


综上，信息增益的计算公式如下

![](http://images.cnitblog.com/blog/571227/201412/121628452909401.png)

其中 `S` 为全部样本集合，`value(T)` 是属性 `T` 所有取值的集合，`v` 是 `T` 的其中一个属性值，`Sv` 是 `S` 中属性 `T` 的值为 `v` 的样例集合，`|Sv|` 为 `Sv` 中所含样例数

在决策树的每一个非叶子结点划分之前，先计算每一个属性所带来的信息增益，选择最大信息增益的属性来划分，因为信息增益越大，区分样本的能力就越强，越具有代表性很显然这是一种自顶向下的贪心策略。以上就是ID3算法的核心思想。